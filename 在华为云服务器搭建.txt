						华为云服务器搭建

购买跳板机用于管理跳扳机名[ecs-proxy] 主机ip192.168.1.252
配置好yum源,去华为云文档查看
安装服务
vsftpd	//ftp服务
ansible	//一件管理
createrepo   //创建yum源.自定义
 bash-com*		//tab补全
配置时间服务器	(chrony) 
cat  /etc/ntp.conf
......
fudge  127.127.1.0 stratum 10

restrict ntp.myhuaweicloud.com nomodify notrap nopeer noquery

server ntp.myhuaweicloud.com iburst   ##华为的内网时间服务器

----
[root@ecs-proxy ~]#vim  /etc/chrony.conf
....

# Use public servers from the pool.ntp.org project.
# Please consider joining the pool (http://www.pool.ntp.org/join.html).
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
server ntp.myhuaweicloud.com iburst		##
:wq
------
停止不用的服务,也可以删除
-------
搭建nginx的rpm包
-----
安装rpm-build软件包
[root@ecs-proxy ~]# yum -y install  rpm-build
生成rpmbuild目录结构
[root@ecs-proxy~]# rpmbuild -ba nginx.spec                //会报错，没有文件或目录
[root@ecs-proxy ~]# ls /root/rpmbuild                    //自动生成的目录结构
BUILD  BUILDROOT  RPMS  SOURCES  SPECS  SRPMS		//如果记得可以自己敲出来
----
准备工作，将源码软件复制到SOURCES目录
[root@ecs-proxy~]# cp nginx-1.12.2.tar.gz /root/rpmbuild/SOURCES/
---
创建并修改SPEC配置文件
[root@ecs-proxy ~]# vim /root/rpmbuild/SPECS/nginx.spec 
Name:nginx                                        #源码包软件名称
Version:1.12.2                                    #源码包软件的版本号
Release:    10                                        #制作的RPM包版本号
Summary: Nginx is a web server software.            #RPM软件的概述    
License:GPL                                        #软件的协议
URL:    www.test.com                                    #网址
Source0:nginx-1.12.2.tar.gz                        #源码包文件的全称
#BuildRequires:                                    #制作RPM时的依赖关系
#Requires:                                        #安装RPM时的依赖关系
%description
nginx [engine x] is an HTTP and reverse proxy server.    #软件的详细描述
%post
useradd nginx                               #非必需操作：安装后脚本(创建账户)
%prep
%setup -q                                #自动解压源码包，并cd进入目录
%build
./configure						#可以添加你要的模块
make %{?_smp_mflags}
%install
make install DESTDIR=%{buildroot}
%files
%doc
/usr/local/nginx/*                    #对哪些文件与目录打包
%changelog
:wq
------
使用配置文件创建RPM包
安装依赖软件包
[root@ecs-proxy ~]# yum -y install  gcc  pcre-devel openssl-devel
----
rpmbuild创建RPM软件包
[root@ecs-proxy ~]#rpmbuild -ba /root/rpmbuild/SPECS/nginx.spec
[root@ecs-proxy ~]# ls /root/rpmbuild/RPMS/x86_64/nginx-1.12.2-10.x86_64.rpm
-----
创建自定义yum源
[root@ecs-proxy ~]#cd  /var/ftp/pub
[root@ecs-proxy ~]#cp  -r  /root/rpmbuild/RPMS/x86_64/nginx-1.12.2-10.x86_64.rpm 
/var/ftp/pub
[root@ecs-proxy ~]#createrepo .
#######
在买一个云主机做镜像方便后面创建云主机
配置yum源(华为内网)
配置时间服务器
停止不用的服务,卸载
配置ngin的自定义域名源

[root@ecs-proxy ~]#vim  yum.repos.d/lve.repo
[name]
name=aaaaa
baseurl=ftp://192.168.1.252/pub
gpgcheck=0
enabled=1
:wq
克隆镜像
###
云上买了个4层调度,会自带高可用
###
自己搭建7层调度器 == 2台云服务器 主机名[root@nginx-proxy7 ~]# (两台以一台为例)
可以yum也可源码安装(调度我是域名安装的)
依赖包
[root@nginx-proxy7 ~]# yum -y install  gcc  pcre-devel openssl-devel
[root@nginx-proxy7 ~]#tar -xf nginx-1.12.2.tar.gz
[root@nginx-proxy7 ~]#cd nginx-1.12.2
[root@nginx-proxy7 ~]#./configure  --with-stream 
[root@nginx-proxy7 ~]#make  && make  install
----
配置文件
[root@nginx-proxy7 ~]#vim /usr/local/nginx/conf/nginx.conf
.....
http{
....
  upstream  webserver{
           server 192.168.1.11;		#定义静态集群
           server 192.168.1.12;

}
    upstream  webdt{
           server 192.168.1.13:8080;		#定义动态集群
           server 192.168.1.14:8080;
           server 192.168.1.15:8080;
}
    server {
        listen       80;
        server_name  localhost;

        #charset koi8-r;

        #access_log  logs/host.access.log  main;

        location / {				#转发静态(nginx)
            root   html;
            index  index.html index.htm;
            proxy_pass http://webserver;
 }
        location ~ .*.jsp$ {			#转发动态(tomcat)
        proxy_pass http://webdt;
        proxy_set_header Host $host;
}
.....
:wq
#########
配置动态服务器和静态服务器
----------
静态服务器(2台以一台为例)(主)(nginx)
[root@webj1 ~]# yum -y install  gcc  pcre-devel openssl-devel
[root@webj1 ~]# yum -y install  nginx
[root@webj1 ~]# vim /usr/local/nginx/conf/nginx.conf
    server {
        listen       80;
        server_name  localhost;

        #charset koi8-r;

        #access_log  logs/host.access.log  main;

        location / {
            root   html/aa;			///定义aa为跟目录方法ceph共享时放静态服务
            index  index.html index.htm;
        }

        #error_page  404              /404.html;

        # redirect server error pages to the static page /50x.html
        #
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
}

[root@webj1 ~]# /usr/local/nginx/sbin/nginx    //启动服务
[root@webj1 ~]#mkdir  /usr/local/nginx/html/aa
[root@webj1 ~]#mv   /usr/local/nginx/html/*  /usr/local/nginx/html/aa/
--------------------
静态服务器(tomcat)
部署Tomcat服务器软件
使用RPM安装JDK环境
[root@webd1 ~]# yum -y install  java-1.8.0-openjdk                //安装JDK
[root@webd1 ~]# yum -y install java-1.8.0-openjdk-headless        //安装JDK
[root@webd1 ~]# java -version                                    //查看JAVA版本
自己准备了一个先前的tomcat的tar包
）安装Tomcat
[root@webd1 ~]# tar -xf  apache-tomcat-8.0.30.tar.gz
[root@webd1 ~]# mv apache-tomcat-8.0.30  /usr/local/tomcat
[root@webd1 ~]# ls /usr/local/tomcat
bin/                                            //主程序目录
lib/                                            //库文件目录
logs/                                          //日志目录  
temp/                                         //临时目录
work/                                        //自动编译目录jsp代码转换servlet
conf/                                        //配置文件目录
webapps/                                        //页面目录
------------------------
启动服务
[root@webd1 ~]# /usr/local/tomcat/bin/startup.sh
-------------
服务器验证端口信息
[root@webd1 ~]# ss -nutlp |grep java        //查看java监听的端口
tcp        0      0 :::8080              :::*                LISTEN      2778/java 
tcp        0      0 :::8009              :::*                LISTEN      2778/java                     
tcp        0      0 ::ffff:127.0.0.1:8005     :::*         LISTEN       2778/java 
-------------
如果检查端口时，8005端口启动非常慢，默认tomcat启动需要从/dev/random读取大量的随机数据，默认该设备生成随机数据的速度很慢，可用使用下面的命令用urandom替换random（非必须操作）。
[root@webd1 ~]# mv /dev/random  /dev/random.bak
[root@webd1 ~]# ln -s /dev/urandom  /dev/random
另外，还可以使用方案二解决：
[root@webd1 ~]# yum install rng-tools
[root@webd1 ~]# systemctl start rngd
[root@webd1 ~]# systemctl enable rngd
----
修改根网页根目录
[root@webd3 ~]# vim /usr/local/tomcat/bb/conf/server.xml 
....
<Context path="" docBase="/usr/local/tomcat/bb" debug="0"/>    ##在这加一行
      <Host name="localhost"  appBase="webapps"
            unpackWARs="true" autoDeploy="true">
:wq
-------------------
移动根目录
[root@webd1 ~]# mkdir /usr/local/tomcat/bb
[root@webd1 ~]# mv /usr/local/tomcat/*  /usr/local/tomcat/bb/
修改Tomcat配置文件
创建测试JSP页面(网上找的)
[root@webd1 ~]# vim  /usr/local/tomcat/bb/webapps/ROOT/test.jsp
<html>
<body>
<center>
Now time is: <%=new java.util.Date()%>            //显示服务器当前时间
</center>
</body>
</html>
:wq

然后重启服务
#####
现在跳板机[ecs-proxy]上配置一个ceph的yum源,我先前准备好的镜像,也可以自己去下载
[root@ecs-proxy ~]#ls  /root/
ceph10.iso
[root@ecs-proxy ~]#mkdir /var/ftp/ceph/
[root@ecs-proxy ~]#vim  /etc/fstab
....
/root/ceph10.iso  /var/ftp/ceph iso9660 defaults 0 0
:wq
[root@ecs-proxy ~]#mount -a
搭建ceph集群(这边我没有做6台用3台做一起了节省机子,分开来就mon和osd地方分开就好)
三台机子node1,node2,node3,node1同时做管理机子(可以分开)
-------
配置无密码连接(包括自己远程自己也不需要密码)，在node1操作。
[root@node1 ~]# ssh-keygen   -f /root/.ssh/id_rsa    -N ''
[root@node1 ~]# for i in 10  18 19 20
 do
     ssh-copy-id  192.168.1.$i
 done
----
修改/etc/hosts并同步到所有主机。
警告：/etc/hosts解析的域名必须与本机主机名一致！！！！
[root@node1 ~]# cat /etc/hosts
....
192.168.1.18    node1
192.168.1.19    node2
192.168.1.20    node3
:wq
------
警告：/etc/hosts解析的域名必须与本机主机名一致！！！！
[root@node1 ~]# for i in client node1  node2  node3
do
scp  /etc/hosts   $i:/etc/
done
----------------
修改所有节点都需要配置YUM源，并同步到所有主机。
[root@node1 ~]# vim /etc/yum.repos.d/ceph.repo
[MON]
name=OSD
baseurl=ftp://192.168.1.252/ceph/MON
gpgcheck=0
enabled=1
[OSD]
name=OSD
baseurl=ftp://192.168.1.252/ceph/OSD
gpgcheck=0
enabled=1
[Tools]
name=Tools
baseurl=ftp://192.168.1.252/ceph/Tools
gpgcheck=0
enabled=1
:wq
---------------------
[root@node1 ~]# yum repolist 
已加载插件：fastestmirror
Loading mirror speeds from cached hostfile
源标识                              源名称                                状态
!MON                                OSD                                       41
!OSD                                OSD                                       28
!Tools                              Tools                                     33
!base/7/x86_64                      CentOS-7 - Base                       10,097
!extras/7/x86_64                    CentOS-7 - Extras                        305
!name                               sss                                        2
!updates/7/x86_64                   CentOS-7 - Updates                       738
repolist: 11,244

-----------------
[root@node1 ~]# for i in  client  node1  node2  node3
do
scp  /etc/yum.repos.d/ceph.repo   $i:/etc/yum.repos.d/
done
-----------------
所有节点主机与真实主机的NTP服务器同步时间。(刚刚镜像设置好了)
提示：默认真实物理机已经配置为NTP服务器。        
----------------------------
准备存储磁盘
云主机上为每个虚拟机准备3块20G磁盘,买就完事(想要多少共享空间看需求)
################
部署ceph集群
沿用练习一，部署Ceph集群服务器，实现以下目标：
安装部署工具ceph-deploy
创建ceph集群
准备日志磁盘分区
创建OSD存储空间
查看ceph状态，验证
----------------------
实现此案例需要按照如下步骤进行。
步骤一：安装部署软件ceph-deploy

在node1安装部署工具，学习工具的语法格式。
[root@node1 ~]#  yum -y install ceph-deploy
[root@node1 ~]#  ceph-deploy  --help
[root@node1 ~]#  ceph-deploy mon --help
------------------------
创建目录
[root@node1 ~]#  mkdir ceph-cluster
[root@node1 ~]#  cd ceph-cluster/
------------------------
部署Ceph集群

给所有节点安装ceph相关软件包。
[root@node1 ceph-cluster]# for i in node1 node2 node3
do
    ssh  $i "yum -y install ceph-mon ceph-osd ceph-mds ceph-radosgw"
done 
------------------------------
创建Ceph集群配置,在ceph-cluster目录下生成Ceph配置文件。
在ceph.conf配置文件中定义monitor主机是谁。
[root@node1 ceph-cluster]# ceph-deploy new node1 node2 node3
------------------------
初始化所有节点的mon服务，也就是启动mon服务（主机名解析必须对）。
[root@node1 ceph-cluster]# ceph-deploy mon create-initial
--------------------------
创建OSD

备注：vdb1和vdb2这两个分区用来做存储服务器的journal缓存盘。
[root@node1 ceph-cluster]# for i in node1 node2 node3
do
     ssh $i "parted /dev/vdb mklabel gpt"
     ssh $i "parted /dev/vdb mkpart primary 1 50%"
     ssh $i "parted /dev/vdb mkpart primary 50% 100%"
 done
-----------------------
磁盘分区后的默认权限无法让ceph软件对其进行读写操作，需要修改权限。
node1、node2、node3都需要操作，这里以node1为例。
[root@node1 ceph-cluster]# chown  ceph.ceph  /dev/vdb1
[root@node1 ceph-cluster]# chown  ceph.ceph  /dev/vdb2
#上面的权限修改为临时操作，重启计算机后，权限会再次被重置。
#我们还需要将规则写到配置文件实现永久有效。
#规则：如果设备名称为/dev/vdb1则设备文件的所有者和所属组都设置为ceph。
#规则：如果设备名称为/dev/vdb2则设备文件的所有者和所属组都设置为ceph。
[root@node1 ceph-cluster]# vim /etc/udev/rules.d/70-vdb.rules
ENV{DEVNAME}=="/dev/vdb1",OWNER="ceph",GROUP="ceph"
ENV{DEVNAME}=="/dev/vdb2",OWNER="ceph",GROUP="ceph"
-------------------------
初始化清空磁盘数据（仅node1操作即可|管理机）。
[root@node1 ceph-cluster]# ceph-deploy disk  zap  node1:vdc   node1:vdd    
[root@node1 ceph-cluster]# ceph-deploy disk  zap  node2:vdc   node2:vdd
[root@node1 ceph-cluster]# ceph-deploy disk  zap  node3:vdc   node3:vdd   
-------------------------------
创建OSD存储空间（仅node1操作即可）
重要：很多在这里会出错！将主机名、设备名称输入错误！！！
[root@node1 ceph-cluster]# ceph-deploy osd create \
 node1:vdc:/dev/vdb1 node1:vdd:/dev/vdb2  
//创建osd存储设备，vdc为集群提供存储空间，vdb1提供JOURNAL缓存，
//每个存储设备对应一个缓存设备，缓存需要SSD，不需要很大
[root@node1 ceph-cluster]# ceph-deploy osd create \
 node2:vdc:/dev/vdb1 node2:vdd:/dev/vdb2
[root@node1 ceph-cluster]# ceph-deploy osd create \
 node3:vdc:/dev/vdb1 node3:vdd:/dev/vdb2 
------------------------------
[root@node1 ceph-cluster]# ceph -s
    cluster f938aa75-5be1-458a-b021-4f0f636d40e4
     health HEALTH_OK
     monmap e1: 3 mons at {node1=192.168.1.18:6789/0,node2=192.168.1.19:6789/0,node3=192.168.1.20:6789/0}
            election epoch 14, quorum 0,1,2 node1,node2,node3
      fsmap e15: 1/1/1 up {0=node3=up:active}
     osdmap e57: 6 osds: 6 up, 6 in
            flags sortbitwise
      pgmap v4832: 320 pgs, 3 pools, 49755 kB data, 682 objects
            391 MB used, 119 GB / 119 GB avail
                 320 active+clean
  client io 13105 B/s rd, 12 op/s rd, 0 op/s wr
-----------------------
部署ceph文件系统

启动mds服务
[root@node1 ceph-cluster]# ceph-deploy mds create node3
#远程nod3，拷贝配置文件，启动mds服务
-----------------------
创建存储池（文件系统由inode和block组成）
[root@node1 ceph-cluster]# ceph osd pool create cephfs_data 128
[root@node1 ceph-cluster]# ceph osd pool create cephfs_metadata 128
[root@node1 ceph-cluster]# ceph osd lspools
0 rbd,1 cephfs_data,2 cephfs_metadata
--------------------------------------
创建文件系统
[root@node1 ceph-cluster]# ceph fs new myfs1 cephfs_metadata cephfs_data
[root@node1 ceph-cluster]# ceph fs ls
name: myfs1, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
--------------------------------
web服务器永久挂载Ceph文件系统（webj1.webj2.webd1,webd2,webd3创建文件系统都需要操作）。
在任意ceph节点，如node1查看ceph账户与密码。
[root@node1 ~]# cat /etc/ceph/ceph.client.admin.keyring 
[client.admin]
    key = AQA0KtlcRGz5JxAA/K0AD/uNuLI1RqPsNGC7zg==     //需要改
--------------
如果没有密钥和配置文件则可以通过admin命令重新发送配置和密钥（备选操作）
[root@node1 ceph-cluster]# ceph-deploy admin node3
//同步配置文件和key
-------------------
动静态一台为例
在静态服务器上挂载ceph,挂载前先把aa目录移出等挂载后在移进去
etc/rc.local是开机启动脚本，任何命令放在该文件中都是开机自启。
[root@webj1 ~]# vim /etc/rc.local 
mount -t ceph 192.168.1.18:6789:/ /usr/local/nginx/html/  -o name=admin,secret=AQCiDd1da+0gEhAAX4v/2fgrHaH1tR60NEflbw==
:wq
[root@webj1 ~]# chmod  +x /etc/rc.local
--------------------
动态服务器上挂载ceph,挂载前先把bb目录移出等挂载后在移进去
[root@webd1~]# vim /etc/rc.local 
mount -t ceph 192.168.1.18:6789:/ /usr/local/tomcat/  -o name=admin,secret=AQCiDd1da+0gEhAAX4v/2fgrHaH1tR60NEflbw==
:wq
[root@webd1~]# chmod +x /etc/rc.local
-------------------------------------
随便一台查看
[root@webj1 ~]# ls /usr/local/nginx/html/
aa  bb
############
有个坑就是rc.local这个文件有2个
[root@webd1 ~]# ll /etc/rc.d/rc.local
lrwxrwxrwx 1 root root 13 9月  29 18:44 /etc/rc.local -> rc.d/rc.local
--------------
[root@webd1 ~]# ll /etc/rc.d/rc.local 
-rw-r--r-- 1 root root 473 9月  14 02:19 /etc/rc.d/rc.local
这两个都要给x权限不然不会启动
########################################
############################################
#################################################
MHA搭建数据库集群
一台控制管理机:mgmguanli  192.168.1.25
一台主数据库:mha1  192.168.1.23  提前装好数据库
两台从数据库:mha2  192.168.1.24  mha3 192.168.1.22	提前装好数据库
-------------------------
准备集群环境自己准备了mha-soft-student(目录)的rpm包
再4台主机上安装包
cd mha-soft-student/  
yum -y install perl-*

在所有机子上做免密登录
[root@mgmguanli ~]#ssh-keygen 
[root@mgmguanli ~]#for i in 25 23 24 22; do ssh-copy-id root@192.168.4.$i; done
-----------------
修改hosts,再把私钥和公钥,都传给所有数据
[root@mgmguanli .ssh]# vim /etc/hosts
192.168.1.22    mha2
192.168.1.23    mha3
192.168.1.24    mha1
192.168.1.25    mgmguanli
:wq
[root@mgmguanli .ssh]#for i in  mha2 mha3 mha1 ;do scp  /etc/hosts  $i:/etc/ ;done
[root@mgmguanli .ssh]#for i in mha2 mha3 mha1 ;do scp  id_rsa  id_rsa.pub $i:/root/.ssh/ ;done
-------------------------------------
基本环境ok
搭建一主多从同步结构
配置主库23
[root@mgmguanli .ssh]#vim /etc/my.cnf
[mysqld]
	server_id=23
	log-bin=master23
:wq
[root@mgmguanli .ssh]#systemctl restart mysqld
[root@mha1 mha-soft-student]# mysql -uroot -p123qqq...A
mysql>grant replication slave  on *.* to repluser@"%" identified by "123qqq...A"
mysql>show master status;
--------------------------------------
配置从库24和22(一台为例)
[root@mha2 ~]#vim /etc/my.cnf
  [mysqld]
	server_id=22
:wq
[root@mha2 ~]#systemctl restart  mysqld
[root@mha2 ~]#mysql -uroot -p123qqq...A	
mysql> change master to master_host="192.168.1.23",master_user="repluser",master_password="123qqq...A",
master_log_file="master51.0000001",master_log_pos=441;
mysql> start slave;
mysql> show slave status\G; //查看状态,io线程yes和sql线程yes
-----------------------------------------------
管理主机配置(提起准备好的mha4mysql-manager-0.56.tar.gz)
[root@mgmguanli ~]#yum -y  install   mha4mysql-node-0.56-0.el6.noarch.rpm
[root@mgmguanli ~]#yum -y install perl-ExtUtils-*
[root@mgmguanli ~]#yum -y install perl-CPAN*
[root@mgmguanli ~]#tar -zxf mha4mysql-manager-0.56.tar.gz
[root@mgmguanli ~]#cd mha4mysql-manager-0.56
[root@mgmguanli ~]#perl Makefile.PL
[root@mgmguanli ~]#make &&make install
------------------------
修改主配置文件
[root@mgmguanli ~]#cat /mha4mysql-manager-0.56/samples/conf/app1.cnf  //路径
[root@mgmguanli ~]#mkdir /etc/mha
[root@mgmguanli ~]#cp /mha4mysql-manager-0.56/samples/conf/app1.cnf  /etc/mha/
[root@mgmguanli ~]#vim /etc/mha/app1.cnf
	[server default] //管理服务默认配置
manager_workdir=/etc/mha  //工作目录
manager_log=/etc/mha/manager.log  //日志文件
master_ip_failover_script=/etc/mha/master_ip_failover   //故障切换脚本

ssh_user=root   //访问ssh服务用户
ssh_port=22   //ssh服务端口


 repl_user=repluser   //主服务器数据同步授权用户
 repl_password=123qqq...A  //密码


user=root  //监控用户
 password=123qqq...A   //密码


[server1]  //指定第1台数据服务器
hostname=192.168.1.22 //服务器ip地址
candidate_master=1  //竞选主服务器
port=3306  //服务端口

[server2]     //指定第2台数据库服务器
hostname=192.168.1.23
candidate_master=1
port=3306

[server3]  //指定第3台数据库服务器
hostname=192.168.1.24
candidate_master=1
port=3306
------------------------------------------
故障创建脚本修改
[root@mgmguanli ~]#vim master_ip_failover
	#!/usr/bin/env perl
.............
	 vim master_ip_failover 
      my $vip = '192.168.1.100/24';  # Virtual IP 虚拟vip//修改虚拟ip
      my $key = "1";[server2]
candidate_master=1
hostname=192.168.4.52
port=3306
      my $ssh_start_vip = "/sbin/ifconfig eth0:$key $vip";
      my $ssh_stop_vip = "/sbin/ifconfig eth0:$key down";
----------------------------------
部署VIP地址临时
[root@mgmguanli ~]#ifconfig eth0:1 192.168.1.100
////有个坑就是自己配置的虚拟ip需要在华为云上注册在白名单并且绑定不让用不了
[root@mgmguanli ~]#ifconfig ech0:1
---------------------------------------------
配置数据节点
安装让价包所有数据库的主机都安装(一台为例)
[root@mha1 ~]# cd mha-soft-student/
[root@mha1 ~]#yum -y install mha4mysql-node-0.56-0.el6.noarch.rpm
---------------------------------------
在所有数据库服务器上做授权
[root@mha1 ~]# mysql -uroot -p123qqq...A
mysql> grant all on *.* to root@"%" identfied by "123qqq...A"
mysql> show grants for root@"%";
mysql>grant replication slave on *.* to repluser@"%" identified by "123qqq...A"
mysql>show grants for repluser@"%";
-------------------------------------------------
 所有数据库配置,修改数据库服务运行参数,开启半同步复制(一台为例)
[root@mha1 ~]#vim /etc/my.cnf 
	........
plugin-load="rpl_semi_sync_master=semisync_master.so;rpl_semi_sync_slave=semisync_slave.so"          //加载模块
rpl_semi_sync_master_enabled=1    //启用master模块
rpl_semi_sync_slave_enabled=1           //启用slave 模块
relay_log_purge=0                    //禁止自动删除中继日志文件
:wq
[root@mha1 ~]#systemctl  restart mysqld 
--------------------------
配置备用服务器mha2和mha3修改 数据库服务运行参数(开启bin_log日志因为52和53也有肯变成主服务器
[root@mha2 ~]#vim /etc/my.cnf 
log_bin=master24
       server_id=24
---------------------------------------------
测试集群环境

  测试ssh配置(在管理主机25操作)管理机
      在管理节点上测试ssh配置
[root@mgmguanli ~]#masterha_check_ssh --conf=/etc/mha/app1.cnf    
            显示 All SSH connection tests passed successfully. 表示成功
          
[root@mgmguanli ~]#masterha_check_repl --conf=/etc/mha/app1.cnf 
              显示MySQL Replication Health is OK. 表示成功
----------------------------------
启动管理服务 25(管理机) (开三台终端一台启动服务,一台看状态,一台看多出来的文件)
  使用 masterha_manager 命令
      --remove_dead_master_conf   //删除宕机主库的配置
      --ignore_last_failover   //忽略xxxx.health文件

[root@mgmguanli ~]#masterha_manager --conf=/etc/mha/app1.cnf --remove_dead_master_conf --ignore_last_failover   //会宕在哪里

   查看状态;masterha_check_status (一台查看)
[root@mgmguanli ~]#masterha_check_status --conf=/etc/mha/app1.cnf
       app1 (pid:2556) is running(0:PING_OK), master:192.168.4.52 表示成功
停止服务 :masterha_stop --conf=/etc/mha/app1.cnf 
-----------------------------
主服务器用户授权
   在主服务器添加访问数据的连接用户
 Myasl>create  database db9;
 Mysql>create  table db9.a(id int);
 Mysql>grant select,insert on db9.* to yaya55@”%” identified by “123qqq...A”;
    
      在2台从服务器查看用户
 Mysql>show grant for yaya55@”%”;
---------
客户端访问
  在客户端连接vip地址访问数据库服务
  ]#mysql -h192.168.4.100 -uyaya55 -p123qqq...A
      在表里面写内容
      两台从查看数据
-----------------------------

测试高可用

       停止mysql服务或者关机  //vip地址会自动跳转
            systemctl s.top mysqld
在启动管理机的mha服务
  客户端访问集群
在从服务器上查看vip地址
    Ifconfig eth0:1
 客户端连接vip地址访问集群
      ]#mysql -h192.168.4.100 -uyaya55 -p123qqq...A
      在表里面写内容
      一台从查看数据
-------------------------------------
修复故障服务器
   配置数据库服务器,具体操作如下
启动mysql服务器
与主服务器数据一致
指定主服务器信息,把坏的数据库配置成现在主数据库的从数据库
启动slave进程
查看状态信息
   配置管理服务器,具体操作如下
修改主配置文件,把server1加回去
测试集群环境
重启管理服务
查看服务状态
#################################
##################################
部署MyCAT读写分离服务(主机准备的Mycat-server-1.6-RELEASE-20161028204710-linux.tar.gz包)
安装软件(两台以一台为例)
[root@mycat1 ~]yum -y install java-1.8.0-openjdk-devel
[root@mycat1 ~]tar  -xf Mycat-server-1.6-RELEASE-20161028204710-linux.tar.gz //免安装解压即可
[root@mycat1 ~]mv mycat/ /user/local/
[root@mycat1 ~]ls  /user/local/mycat/
[root@mycat1 mycat]# cd /usr/local/mycat/conf/
[root@mycat1 conf]# mv schema.xml schema.xml.bak   //备份
[root@mycat1 conf]# cp -r  schema.xml.bak schema.xml   
[root@mycat1 conf]# vim server.xml
<user name="yaya66">			//修改用户需要在主数据库授权
                <property name="password">123qqq...A</property>	//密码
                <property name="schemas">mydb</property>

                <!-- 表级 DML 权限设置 -->
                <!--            
                <privileges check="false">
                        <schema name="TESTDB" dml="0110" >
                                <table name="tb01" dml="0000"></table>
                                <table name="tb02" dml="1111"></table>
                        </schema>
                </privileges>           
                 -->
        </user>

        <user name="read">	//只读用户,需要主数据库授权
                <property name="password">123qqq...A</property>	//密码
                <property name="schemas">mydb</property>
                <property name="readOnly">true</property>


[root@mycat1 conf]# vim schema.xml
	<?xml version="1.0"?>
<!DOCTYPE mycat:schema SYSTEM "schema.dtd">
<mycat:schema xmlns:mycat="http://io.mycat/">
		//mydb是虚拟数据库
  <schema name="mydb" checkSQLschema="false" sqlMaxLimit="100" dataNode="dn1" >
  </schema>
  <dataNode name="dn1" dataHost="db9" database="mydb" />
  <dataHost name="db9" maxCon="1000" minCon="10" balance="3"
        writeType="0" dbType="mysql" dbDriver="native" switchType="1"  slaveThreshold="100">
    <heartbeat>select user()</heartbeat>
    <!-- can have multi write hosts -->
    <writeHost host="mha3" url="192.168.1.100:3306" user="yaya66"  //
           password="123qqq...A">
      <!-- can have multi read hosts -->
      <readHost host="mha2" url="192.168.1.22:3306" user="read" password="123qqq...A" />
      <readHost host="mha1" url="192.168.1.24:3306" user="read" password="123qqq...A" />
    </writeHost>
  </dataHost>
</mycat:schema>
~             
这里是读写分离的,其中分片的删除了
----------------------------------------------------------
[root@mycat1 conf]#/usr/local/mycat/bin/mycat start
[root@mycat1 conf]#ss -untlp | grep 8066
tcp    LISTEN     0      100      :::8066                 :::*                   users:(("java",pid=2368,fd=78))
配置完成以后连接 mycat 查询 保证两台mycat都可以实现如下的效果同时可以写入操作，这里只展示一台
[root@mycat1 conf]# mysql -uyaya66 -p123qqq...A -h192.168.1.21 -P 8066 -e 'select @@hostname;'
mysql: [Warning] Using a password on the command line interface can be insecure.
+------------+
| @@hostname |
+------------+
| mha2     |
+------------+
[root@mycat1 conf]# mysql -uyaya66 -p123qqq...A -h192.168.1.21 -P 8066 -e 'select @@hostname;'
mysql: [Warning] Using a password on the command line interface can be insecure.
+------------+
| @@hostname |
+------------+
| mha3     |
+------------+
----------------------------------------
或者将Mycat的日志模式改为debug模式，执行sql后，到日志查看执行的节点ip就知道是不是自己设置的读节点
[root@Mycat1 ~]# cd /usr/local/mycat/conf
[root@Mycat1 conf]# vim log4j2.xml +25
...
        <asyncRoot level="debug" includeLocation="true">
...
[root@Mycat1 conf]# tail -f ../logs/mycat.log
############################
################################
keeplived+haproxy服务器的详细配置，haproxy给两台mycat做均衡负载，keeplived给haproxy做高可用（以一台为例）
[root@haproxy2 ~]#yum -y install haproxy
[root@haproxy2 ~]# vim /etc/haproxy/haproxy.cfg
.....................

#---------------------------------------------------------------------
# main frontend which proxys to the backends
#---------------------------------------------------------------------
listen mycat_3306 *:3306
    mode    tcp        # mysql 得使用 tcp 协议
    option  tcpka      # 使用长连接
    balance leastconn  # 最小连接调度算法
    server  mycat1 192.168.1.21:8066 check inter 3000 rise 1 maxconn 1000 fall 3
    server  mycat2 192.168.1.30:8066 check inter 3000 rise 1 maxconn 1000 fall 3

：wq
------------------------
安装keeplived做高可用一台为例（设为双主模式两个vip）
[root@mycat1 conf]#yum -y install keepalived
! Configuration File for keepalived

global_defs {
   router_id mysql01
}

vrrp_script chk_haproxy {
   script "killall -0 haproxy"     # cheaper than pidof
   interval 2                      # check every 2 seconds
}

vrrp_instance mycat_1 {	#第一个为MASTER第二台反过来配置
    state MASTER
    interface eth0
    virtual_router_id 51
    priority 255
    ! nopreempt
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass 1234
    }
    virtual_ipaddress {
        192.168.1.138/24 brd 192.168.1.255 dev eth0 label eth0:1
    }
    track_script {
       chk_haproxy weight=0    # +2 if process is present
    }
}

vrrp_instance mycat_2 { #第二个为从第二台反过来配置
    state BACKUP
    interface eth0
    virtual_router_id 52
    priority 200
    nopreempt
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass 2345
    }
    virtual_ipaddress {
        192.168.1.205/24 brd 192.168.1.255 dev eth0 label eth0:2
    }
    track_script {
       chk_haproxy weight=0    # +2 if process is present
    }
}

----------------------
查看IP是否生成，还要去华为云白名单注册
[root@haproxy2 ~]# ifconfig
eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.1.26  netmask 255.255.255.0  broadcast 192.168.1.255
        inet6 fe80::f816:3eff:fe5b:62a  prefixlen 64  scopeid 0x20<link>
        ether fa:16:3e:5b:06:2a  txqueuelen 1000  (Ethernet)
        RX packets 764  bytes 77130 (75.3 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 722  bytes 90791 (88.6 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

eth0:1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.1.138  netmask 255.255.255.0  broadcast 192.168.1.255
        ether fa:16:3e:5b:06:2a  txqueuelen 1000  (Ethernet)
-------------------
测试读写分离是否ok，轮训且可以写入说明正确
[root@haproxy2 ~]#mysql -uyaya66 -p123qqq...A -h192.168.1.138
MySQL [(none)]> select @@hostname
    -> ;
+------------+
| @@hostname |
+------------+
| mha3     |
+------------+
1 row in set (0.01 sec)

MySQL [(none)]> select @@hostname;
+------------+
| @@hostname |
+------------+
| mha2     |
+------------+
————————————————

###################
搭建Redis集群给数据库做缓存一共6台（一台为例）
自己准备的tar包redis-4.0.8.tar.gz
root@redis1主机同时做管理主机，也可以在搭建一台
[root@redis1 ~]# yum -y install rubygems        //下载 gem装包工具,来解压后缀为gem的包down
[root@redis1 ~]#gem install redis-3.2.1.gem  	//解压包，包自己准备
[root@redis1 ~]# mkdir /root/bin	//创建目录
[root@redis1 src]# cp redis-trib.rb /root/bin    	//拷贝集群管理脚本
[root@redis1~]# chmod +x /root/bin/redis-trib.rb        //添加执行程序//方便使用


-------------- 
vim redis.sh  //一键部署环境
#!/bin/bash
cd redis/
tar -xf redis-4.0.8.tar.gz
cd redis-4.0.8/
 make &&make install
./utils/install_server.sh <<EOF








EOF   //这是初始化
--------------------------
修改配置文件，用ansible一件部署查看就成功,端口可以都用默认的6379
vim 6379.conf
edis/redis.cnf
815	cluster-enabled yes
815	 cluster-config-file nodes-6379.conf
815	cluster-node-timeout 5000
:wq
]#/etc/init.d/redis_6379 stop
]#/etc/init.d/redis_6379 start
]#netstat -anutlp | grep redis
tcp        0      0 192.168.1.55:16355      0.0.0.0:*               LISTEN      4466/redis-server 1 
tcp        0      0 192.168.1.55:6355       0.0.0.0:*               LISTEN      4466/redis-server 1

-----------------------
vim  redis.yml
---
- name:  redis
  hosts:  redis
  tasks:
  - name: redis
    template:
      src: /root/6379.conf
      dest: /etc/redis/
      owner: root
      group: root
      mode: 0644

-----------------
创建集群，集群搭建好了
redis-trib.rb create  --replicas 1   192.168.1.27:6327 192.168.1.28:6328  192.168.1.53:6330 192.168.1.188:6329 192.168.1.186:6331 192.168.1.33:6332

[root@redis1 ~]# redis-trib.rb info 192.168.1.27:6327
192.168.1.27:6327 (a7caf80f...) -> 0 keys | 5461 slots | 1 slaves.
192.168.1.53:6330 (0529a32a...) -> 0 keys | 5461 slots | 1 slaves.
192.168.1.28:6328 (e13f2ce3...) -> 0 keys | 5462 slots | 1 slaves.
[OK] 0 keys in 3 masters.
-------------------------







